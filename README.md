# Отчет по первой лабораторной работе №1
## Выполнили  студенты 415г:
### Боковой В.С
### Ершов А.А
### Новая Д.А

## Теоретическая база

### Свёрточные нейронные сети (CNN)

Свёрточные нейронные сети (Convolutional Neural Networks, CNN) являются одним из самых эффективных методов для обработки и классификации изображений. Это специализированный тип нейронных сетей, который использует операции свёртки для извлечения признаков из изображений. Свёртки позволяют сети автоматически обучаться распознавать различные характеристики изображения, такие как края, текстуры и формы, что делает CNN особенно полезными для задач компьютерного зрения.

Основные компоненты свёрточной нейронной сети включают:
1. **Свёрточные слои (Convolutional Layers)** — применяют фильтры (ядра свёртки) для выявления локальных признаков в изображении. Они отвечают за извлечение базовых признаков, таких как края или углы.
2. **Слои активации (Activation Layers)** — обычно используется функция активации ReLU (Rectified Linear Unit), которая добавляет нелинейность в модель, что позволяет сети обучать более сложные представления.
3. **Слои подвыборки (Pooling Layers)** — используется операция подвыборки (чаще всего MaxPooling), которая уменьшает размерность выходных данных, сокращая вычислительные затраты и повышая инвариантность к небольшим изменениям в изображении.
4. **Полносвязные слои (Fully Connected Layers)** — соединяют все выходы из предыдущих слоёв, производя окончательную классификацию.

Свёрточные нейронные сети эффективно решают задачи классификации, так как они могут автоматически извлекать признаки из изображений, обходя необходимость вручную задавать эти признаки, как в традиционных алгоритмах машинного обучения.

### AlexNet

**AlexNet** — это одна из наиболее известных и широко используемых архитектур CNN, предложенная в 2012 году Алексом Крижевским и его коллегами для участия в соревновании ImageNet. Эта модель значительно улучшила результаты классификации изображений и стала основой для многих последующих моделей. AlexNet состоит из 8 слоёв: 5 свёрточных слоёв и 3 полносвязанных слоя. Особенностями архитектуры являются:

- **Использование больших фильтров свёртки** (например, 11x11 в первом слое), что позволяет выделять более широкие контексты на изображении.
- **Использование функции активации ReLU**, которая ускоряет обучение, улучшая сходимость модели по сравнению с традиционными функциями активации, такими как сигмоида или tanh.
- **Снижение размерности с помощью слоёв MaxPooling**, что помогает уменьшить вычислительную сложность и повысить инвариантность модели.
- **Регуляризация с использованием Dropout** для уменьшения переобучения в полносвязанных слоях.

AlexNet значительно повысил точность классификации в задачах компьютерного зрения и стал основой для множества более сложных архитектур.

### Оптимизация в нейронных сетях

Для обучения нейронных сетей используется метод **обратного распространения ошибки (backpropagation)**, который позволяет обновлять веса сети с помощью градиентного спуска. Однако стандартный метод градиентного спуска может быть недостаточно эффективным, особенно при работе с большими моделями и сложными задачами. Для решения этой проблемы были разработаны различные методы оптимизации, такие как **Adam** и **AdaDelta**.

#### Оптимизатор Adam

**Adam (Adaptive Moment Estimation)** — это один из самых популярных оптимизаторов для обучения нейронных сетей, который сочетает в себе преимущества двух методов: адаптивных методов (таких как AdaGrad) и методов, использующих моменты (таких как RMSprop). Adam использует два значения для каждого параметра: первое — это среднее значение градиента, а второе — среднеквадратичное значение градиента. Эти параметры обновляются с помощью адаптивных шагов, что помогает ускорить сходимость и делает обучение более стабильным.

Основные преимущества Adam:
- Адаптивная коррекция скорости обучения для каждого параметра.
- Быстрая сходимость.
- Устойчивость к шуму в данных.

#### Оптимизатор AdaSmoothDelta

**AdaSmoothDelta** — это усовершенствованный вариант оптимизатора **AdaDelta**, который также использует адаптивное изменение шагов обновления для каждого параметра. В отличие от стандартных методов, таких как Adam, AdaDelta ориентирован на более гладкую коррекцию скорости обучения, что делает его более стабильным на разных этапах обучения.

Основные особенности AdaSmoothDelta:
- Использует экспоненциальные скользящие средние для вычисления и корректировки шага.
- Адаптивно регулирует шаг обновления для каждого параметра, что помогает ускорить обучение.
- Меньше чувствителен к выбору гиперпараметров.

### Датасет

В данном проекте используется набор данных **Stanford Cars Dataset**, который содержит изображения автомобилей различных моделей. Каждый класс в наборе данных соответствует определённой модели автомобиля, и задача заключается в классификации изображения в одну из этих моделей. Этот набор данных включает более 16,000 изображений и 196 классов.

Данные представлены в виде изображений с размерами 255x255 пикселей, и для их обработки необходимо произвести ресайзинг до 227x227 пикселей (в соответствии с требованиями AlexNet). Изображения имеют различные условия освещенности, фоны и перспективы, что добавляет сложности задаче классификации.

### Процесс обучения и тестирования

Процесс обучения модели включает несколько этапов:
1. **Предобработка данных**: Изображения изменяются до нужных размеров и нормализуются с использованием стандартных значений (средние и стандартные отклонения для каждого канала RGB).
2. **Обучение**: Сеть обучается с использованием обучающих данных, где каждое изображение проходит через сеть, и вычисляется ошибка, которая затем используется для обновления весов модели с помощью оптимизатора.
3. **Тестирование**: После завершения обучения сеть проверяется на тестовых данных, чтобы оценить её способность обобщать знания и классифицировать новые изображения.

Процесс обучения повторяется в несколько эпох, и на каждом этапе оценивается точность модели. Тестовая точность позволяет оценить, насколько хорошо модель справляется с задачей классификации.


## 3. Результат выполнения программы
```
Обучение с AdaSmoothDelta
Эпоха 1, Время обучения: 193.16c., Потери: 5.7546, Точность: 0.55%
Эпоха 2, Время обучения: 197.01c., Потери: 5.3471, Точность: 0.52%
Эпоха 3, Время обучения: 185.59c., Потери: 5.3282, Точность: 0.65%
Эпоха 4, Время обучения: 185.0c., Потери: 5.3159, Точность: 0.54%
Эпоха 5, Время обучения: 185.02c., Потери: 5.2961, Точность: 0.69%
Эпоха 6, Время обучения: 184.01c., Потери: 5.2636, Точность: 0.97%
Эпоха 7, Время обучения: 184.18c., Потери: 5.1907, Точность: 1.23%
Эпоха 8, Время обучения: 184.23c., Потери: 5.1312, Точность: 1.60%
Эпоха 9, Время обучения: 184.68c., Потери: 5.0941, Точность: 1.78%
Эпоха 10, Время обучения: 184.18c., Потери: 5.0518, Точность: 2.04%
Тестовая точность: 1.84%

-----------------------------------

Обучение с Adam
Эпоха 1, Время обучения: 185.64c., Потери: 5.7666, Точность: 0.60%
Эпоха 2, Время обучения: 184.99c., Потери: 5.3494, Точность: 0.52%
Эпоха 3, Время обучения: 184.54c., Потери: 5.3286, Точность: 0.47%
Эпоха 4, Время обучения: 183.66c., Потери: 5.3123, Точность: 0.54%
Эпоха 5, Время обучения: 184.66c., Потери: 5.3073, Точность: 0.54%
Эпоха 6, Время обучения: 183.31c., Потери: 5.2796, Точность: 0.80%
Эпоха 7, Время обучения: 182.94c., Потери: 5.2289, Точность: 1.07%
Эпоха 8, Время обучения: 183.85c., Потери: 5.1582, Точность: 1.22%
Эпоха 9, Время обучения: 183.96c., Потери: 5.1060, Точность: 1.53%
Эпоха 10, Время обучения: 184.38c., Потери: 5.0606, Точность: 1.79%
Тестовая точность: 2.14%
```

## 4. Использованные источники

Крижевский А. В., Суцкевер И. В., Хинтон Г. Е. Классификация ImageNet с глубокими сверточными нейронными сетями // Коммуникации АКМ. – 2017. – Т. 60, No6. – С. 84–90.

Статья с описанием работы EfficientNet [электронный ресурс]. – https://neurohive.io/ru/vidy-nejrosetej/alexnet-svjortochnaja-nejronnaja-set-dlja-raspoznavanija-izobrazhenij/ (дата обращения: 09.03.2025).

Описание оптимизаторов [электронный ресурс]. – Режим доступа: https://proproprogs.ru/ml/ml-optimizatory-gradientnyh-algoritmov-rmsprop-adadelta-adam-nadam (дата обращения: 25.12.2024).

Stanford Cars Dataset [электронный ресурс]. – Режим доступа: http://ai.stanford.edu/~jkrause/cars/car_dataset.html (дата обращения: 25.12.2024).

Документация PyTorch [Электронный ресурс]. – Режим доступа: https://pytorch.org/docs/stable/ (дата обращения: 25.12.2024).